{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sift import SIFT\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "# 构建视觉词袋\n",
    "def build_vocabulary(data, k, max_iters=100):\n",
    "    # 迭代最大次数为100\n",
    "    # 随机初始化聚类中心\n",
    "    centroids = data[np.random.choice(len(data), k, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # 将每个数据点分配到最近的聚类中心\n",
    "        distances = np.linalg.norm(data[:, np.newaxis, :] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # 更新聚类中心\n",
    "        for i in range(k):\n",
    "            cluster_points = data[labels == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                centroids[i] = np.mean(cluster_points, axis=0)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "# 视觉词频统计\n",
    "def calculate_word_frequencies(descriptors, vocabulary):\n",
    "    distances = np.linalg.norm(descriptors[:, np.newaxis, :] - vocabulary, axis=2)\n",
    "    closest_words = np.argmin(distances, axis=1)\n",
    "    word_frequencies = np.bincount(closest_words, minlength=len(vocabulary))\n",
    "    total_sum = sum(word_frequencies)\n",
    "    frequencies = [element / total_sum for element in word_frequencies]\n",
    "    return frequencies\n",
    "\n",
    "# 训练分类器SVM\n",
    "def train_classifier(X, y):\n",
    "    scaler = StandardScaler() # 每个特征的均值为0，标准差为1\n",
    "    clf = make_pipeline(scaler, SVC(kernel='linear')) # clf包括了scaler放缩和svm预测两个步骤\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取训练集描述符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|          | 0/2583 [00:00<?, ? image/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 2583/2583 [24:14<00:00,  1.78 image/s]\n"
     ]
    }
   ],
   "source": [
    "train_folder = './data/training/'\n",
    "test_folder = './data/testing/'\n",
    "my_train_folder = './data/my_training/'\n",
    "my_test_folder = './data/my_testing/'\n",
    "\n",
    "descriptors_list = [] # 描述符列表, shape为（图片数量 描述符数量 描述符维度）\n",
    "labels_list = [] # 标签列表, shape为（图片数量）\n",
    "\n",
    "# 提取描述符\n",
    "for root, dirs, files in os.walk(train_folder):\n",
    "    # 随机抽取十分之一的文件\n",
    "    sampled_files = random.sample(files, len(files) // 10)\n",
    "    for file in tqdm(sampled_files, desc=\"Processing Images\", unit=\" image\"):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = int(file[0])  # 从文件名中提取标签\n",
    "\n",
    "            origin_img = plt.imread(img_path)\n",
    "            if len(origin_img.shape) == 3:\n",
    "                img = origin_img.mean(axis=-1)\n",
    "            else:\n",
    "                img = origin_img\n",
    "            keyPoints, descriptors = SIFT(img)\n",
    "\n",
    "            descriptors_list.append(np.array(descriptors))\n",
    "            labels_list.append(label)\n",
    "\n",
    "unique_labels = np.unique(labels_list)\n",
    "combined_descriptors = [np.concatenate([desc for desc, label in zip(descriptors_list, labels_list) if label == unique_label], axis=0) for unique_label in unique_labels] # 按类别组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取测试集描述符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 127/127 [01:12<00:00,  1.75 image/s]\n"
     ]
    }
   ],
   "source": [
    "# 提取测试集描述符\n",
    "test_descriptors_list = [] # 描述符列表\n",
    "test_labels_list = [] # 标签列表\n",
    "\n",
    "# 提取描述符\n",
    "for root, dirs, files in os.walk(test_folder):\n",
    "    # 随机抽取十分之一的文件\n",
    "    sampled_files = random.sample(files, len(files) // 20)\n",
    "\n",
    "    for file in tqdm(sampled_files, desc=\"Processing Images\", unit=\" image\"):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = int(file[0])  # 从文件名中提取标签\n",
    "\n",
    "            origin_img = plt.imread(img_path)\n",
    "            if len(origin_img.shape) == 3:\n",
    "                img = origin_img.mean(axis=-1)\n",
    "            else:\n",
    "                img = origin_img\n",
    "            keyPoints, descriptors = SIFT(img)\n",
    "\n",
    "            test_descriptors_list.append(np.array(descriptors))\n",
    "            test_labels_list.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用训练集描述符生成词袋，并对训练集进行词频统计，训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12085, 128)\n",
      "(10486, 128)\n",
      "(10591, 128)\n",
      "(11748, 128)\n",
      "(11536, 128)\n"
     ]
    }
   ],
   "source": [
    "classes_num = 5 # 类别数量\n",
    "vocabulary_size = 40 # 词袋数量\n",
    "\n",
    "# 构建词袋\n",
    "all_descriptors = np.vstack(descriptors_list)\n",
    "\n",
    "total_vocabulary = np.empty((0, 128))\n",
    "\n",
    "for descriptors_small_list in combined_descriptors:\n",
    "    print(descriptors_small_list.shape)\n",
    "    vocabulary = build_vocabulary(descriptors_small_list, vocabulary_size)\n",
    "    total_vocabulary = np.vstack([total_vocabulary, vocabulary])\n",
    "\n",
    "vocabulary = total_vocabulary\n",
    "\n",
    "# 每张图片的视觉词频统计\n",
    "X_train = []\n",
    "\n",
    "for descriptors in descriptors_list:\n",
    "    word_frequencies = calculate_word_frequencies(descriptors, vocabulary)\n",
    "    X_train.append(word_frequencies)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(labels_list)\n",
    "\n",
    "# 训练分类器\n",
    "clf = train_classifier(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对测试集进行词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算测试集的视觉单词频率\n",
    "X_test = []\n",
    "for descriptors in test_descriptors_list:\n",
    "    # print(descriptors.shape)\n",
    "    # print(vocabulary.shape)\n",
    "    word_frequencies = calculate_word_frequencies(descriptors, vocabulary)  # 使用训练集的词典\n",
    "    X_test.append(word_frequencies)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(test_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60       529\n",
      "           2       0.55      0.54      0.55       478\n",
      "           4       0.65      0.64      0.64       525\n",
      "           6       0.69      0.70      0.70       537\n",
      "           8       0.65      0.58      0.61       514\n",
      "\n",
      "    accuracy                           0.62      2583\n",
      "   macro avg       0.62      0.62      0.62      2583\n",
      "weighted avg       0.62      0.62      0.62      2583\n",
      "\n",
      "Accuracy: 0.29\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.44      0.33        25\n",
      "           2       0.14      0.13      0.13        23\n",
      "           4       0.40      0.23      0.29        35\n",
      "           6       0.32      0.31      0.31        26\n",
      "           8       0.37      0.39      0.38        18\n",
      "\n",
      "    accuracy                           0.29       127\n",
      "   macro avg       0.30      0.30      0.29       127\n",
      "weighted avg       0.31      0.29      0.29       127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计识别准确率\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def test_classifier(clf, X_test, y_test):\n",
    "    # 预测测试集标签\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    # 打印分类报告\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print('Classification Report:\\n', report)\n",
    "\n",
    "# 使用已经训练好的分类器进行测试\n",
    "test_classifier(clf, X_train, Y_train)\n",
    "test_classifier(clf, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果\n",
    "1. 20*5：49%+31%\n",
    "2. 25*5：52%+33%\n",
    "3. 30*5：55%+31%\n",
    "4. 35*5：59%+31%\n",
    "5. 40*5：62%+29%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
